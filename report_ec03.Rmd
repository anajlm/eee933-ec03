---
title: 'Estudo de Caso 3 - Comparação de desempenho de duas configurações de um algoritmo de otimização'
author: "Ana Júlia de Lima Martins, Antônio Carlos da Anunciação, Melchior Augusto Syrio de Melo"
date: "24 de dezembro de 2024"
output:
  pdf_document:
    fig_caption: yes
  html_document:
    df_print: paged
---

```{r setup,results='hide',warning=FALSE,echo=FALSE}
# A few initial definitions just to make sure all required packages are installed. Change as needed.
# NOTE: It may echo some weird messages to the PDF on the first compile (package installation messages). Run twice and the problem will (hopefully) go away.
suppressPackageStartupMessages({
if (!require(ggplot2, quietly = TRUE)){
      install.packages("ggplot2")
      }
# if (!require(devtools, quietly = TRUE)){
#       install.packages("devtools")
#       }
 if (!require(broom, quietly = TRUE)){
       devtools::install_github("dgrtwo/broom")
      }
# if (!require(GGally, quietly = TRUE)){
#       install.packages("GGally")
#       }
if (!require(dplyr, quietly = TRUE)){
      install.packages("dplyr")
}
if (!require(reshape2, quietly = TRUE)){
      install.packages("reshape2")
}
if (!require(multcomp, quietly = TRUE)){
      install.packages("multcomp")
}})
```
## Resumo

Este relatório descreve uma análise estatística do desempenho de duas configurações distintas de um algoritmo de otimização baseado em evolução diferencial (DE). Neste experimento, aplicou-se um Experimento Completamente Aleatorizado com Blocos (RCBD) com o intuito de avaliar o desempenho das duas configurações em diferentes instâncias do problema. A análise teve como objetivo identificar se há diferenças estatisticamente significativas no desempenho médio das configurações, determinando qual delas apresenta o melhor desempenho médio e avaliando a magnitude das diferenças encontradas. Por fim, os resultados são apresentados, proporcionando recomendações baseadas nos achados experimentais.

## 1. Design do Experimento

A análise será conduzida a partir de um Experimento Completamente Aleatorizado com Blocos (RCBD), em que as instâncias do algoritmo são consideradas os blocos, e as configurações do algoritmo representam os tratamentos (níveis do fator de interesse). O objetivo é verificar se há diferenças estatisticamente significativas no desempenho médio entre as configurações do algoritmo, levando em conta a variabilidade atribuída às instâncias (blocos).

## 1.1. Hipótese

Para esse experimento, estamos interessados em investigar se há alguma diferença no desempenho médio do algoritmo quando equipado com diferentes configurações, para a classe de problemas de interesse. Portanto, para realizar a análise estatística, estabelecemos as seguintes hipóteses:

- **Hipótese Nula (\(H_0\))**: Não há diferença no desempenho médio entre as configurações, isto é, não existe diferença no tamanho dos efeitos $\tau_i$.

$$\begin{cases} H_0: \tau_i = 0, \ \forall i \in \{1,2,...,a\}\end{cases}$$

- **Hipótese Alternativa (\(H_a\))**: Existe pelo menos uma configuração que apresenta um efeito significativamente diferente de zero, ou seja, com um desempenho superior.

$$\begin{cases} H_1: \exists \ \tau_i \neq 0 \end{cases}$$

Se decidirmos pela rejeição da hipótese nula e as premissas do teste forem validadas, precisaremos determinar qual configuração em termos de desempenho médio. Para responder a essa pergunta, pode-se realizar uma comparação todos contra todos, utilizando o teste de Tukey devido a sua sensibilidade superior ao fazer esse tipo de comparação.

## 1.2. Tamanho amostral

Nesta seção, discutimos a definição do número de instâncias e de repetições necessárias para o experimento, a fim de obter um poder do teste de pelo menos $\pi^* = 0,8$, para detectar diferenças iguais ou superiores a um tamanho de efeito minimamente relevante $d^* = 0,5$, com nível de significância $\alpha = 0,05$.

### 1.2.1. Estimando o número de instâncias (blocos)

Sob a hipótese alternativa H1, a estatística $t_0$ segue uma distribuição t não central (Mathews, 2010) com parâmetro de não centralidade dado por:

$$ncp = \frac{(\mu_D - \mu_0) \sqrt{N}}{\hat{\sigma}_{\Phi}} = \frac{\delta \sqrt{N}}{\hat{\sigma}_\Phi} = d \sqrt{N}.$$
em que $N$ é o número de instâncias necessárias.

Assumindo uma medida de relevância mínima  $d^* = |\delta^*| / \sigma_{\Phi}$, o poder do teste $\pi^*$ é dado pela integral da distribuição t não central com $ncp^* = d^* \sqrt{N}$ sobre os valores de $t_0$ para os quais a hipótese nula $H_0$ é rejeitada (Campelo et. al., 2019):

$$\pi^* = 1 - \beta^* = 1 - \int_{t = t^{(N-1)}_{\alpha/2}}^{t^{(N-1)}_{1-\alpha/2}} \left[ t^{(N-1)}_{\,|\, \text{ncp}^*|} \right] dt$$
Finalmente, o número de instâncias pode ser calculado como o menor inteiro $N$ tal que o poder do teste $\pi^*$ seja igual ou maior que o poder desejado. No caso da hipótese alternativa unilateral, temos que:

$$N^* = \min N \, \bigg| \, t^{(N-1)}_{1-\alpha} \leq t^{(N-1)}_{\beta^* \, ; \, |\text{ncp}^*|}$$


```{r blocos,message=FALSE}
d = 0.5        # Mínima diferença de importância prática (padronizada)
alpha <- 0.05  # Nível de significância
p <- 1 - alpha # Nível de confiança

n <- 2
power <- 0
while (power < 0.8)
{
  df <- n - 1         # n - 1 graus de liberdade
  ncp <- d * sqrt(n)  # Non-centrality parameter

  power <- pt(qt(p,df), df, ncp=ncp, lower.tail=FALSE)
  n <- n + 1
}
n
power
```


### 1.2.2. Estimando o número de repetições por bloco

A abordagem proposta para calcular o número de repetições para o RCBD é similar ao procedimento para um CRD (_Completely Randomized Design_).

Para o CRD, o parâmetro de não-centralidade é definido como:

$$\delta^2 = \frac{n \sum_{i=1}^a (\mu_i - \mu)^2}{\sigma^2}$$
A variabilidade intra-grupo, necessária para calcular o _ncp_, corresponde a variância residual estimada, i.e., a variabilidade não explicada pelo fator experimental e pelo fator bloqueado. Neste experimento, essa variabilidade foi estimada a partir de um estudo piloto com 30 repetições por bloco.

Uma vez definido o _ncp_, a estatística F segue a seguinte distribuição:

$$F \sim\begin{cases}  F_{a-1, a(n-1)} & \text{sob } H_0 \\ F_{a-1, a(n-1), \delta^2} & \text{sob } H_1 \end{cases}$$

O valor crítico $F^*$ para rejeitar $H_0$ com nível de significância $\alpha = 0,05$ é dado por:

$$F^* = F_{a-1, a(n-1), \alpha}$$

Esse valor pode ser calculado usando o seguinte comando em R:

```r
F.crit <- qf(alpha, a-1, a*(n-1), lower.tail = FALSE) # syntax
```

Por definição, o poder do teste é a probabilidade de rejeitar a hipótese nula ($H_0$) quando a hipótese alternativa $H_1$ é verdadeira:

$$\text{Poder} = P(\text{Rejeitar } H_0 | H_1 \text{ é verdadeira}).$$

Isso é equivalente a:

$$P(F(a-1, a(n-1), \delta^2) \geq F^*)$$
Sendo assim, o poder do teste pode ser calculado usando o seguinte comando:

```r
power <- pf(F.crit, a-1, a*(n-1), ncp = ncp, lower.tail = FALSE) # syntax
```

A estratégia consiste em variar o número de repetições $n$, considerando $a = 2$ níveis do fator de interesse, até obter um poder do teste igual ou superior ao poder desejado ($\pi \geq 0,8$)


```{r repeticoes,message=FALSE}
# Carregar a tabela de resultados
estudo_piloto <- read.csv("estudo_piloto_4.csv")

# Criar um dicionário (ou lista) para armazenar os resultados
estatisticas_piloto <- list()

# Obter todas as dimensões únicas
dimensoes <- unique(estudo_piloto$Dimensao)

# Calcular variância e desvio padrão para cada dimensão
for (dim in dimensoes) {
  # Filtrar os dados para a dimensão atual
  dados_dimensao <- subset(estudo_piloto, Dimensao == dim)

  # Calcular variância e desvio padrão de Fbest
  variancia <- var(dados_dimensao$Fbest)
  desvio_padrao <- sd(dados_dimensao$Fbest)

  # Salvar no dicionário (como uma lista com os dois valores)
  estatisticas_piloto[[as.character(dim)]] <- list(
    Variancia = variancia,
    DesvioPadrao = desvio_padrao
  )
}

estatisticas_piloto_df <- data.frame(
  Dimensao = as.numeric(names(estatisticas_piloto)),
  Variancia = sapply(estatisticas_piloto, function(x) x$Variancia),
  DesvioPadrao = sapply(estatisticas_piloto, function(x) x$DesvioPadrao)
)
```


```{r repeticoes2,message=FALSE}
alpha <- 0.05 # nível de significância
d <- 0.5
a <- 2 # Niveis do fator
n_max <- 100 # Número máximo de epetições por instância

# Dataframe para armazenar os resultados
repeticoes <- data.frame(
  Dimensao = integer(),
  Repeticoes = integer(),
  Poder = numeric()
)

for (dim in dimensoes) {
  # Calcula n para cada dimensão para atingir poder >= 0.8
  n <- 2
  power <- 0
  sd <- estatisticas_piloto_df$DesvioPadrao[estatisticas_piloto_df$Dimensao == dim]

  while (power < 0.8)
  {
    df1 <- a - 1 # Graus de liberdade do numerador
    df2 <- a * (n - 1) # Graus de liberdade do denominador
    
    # Calcula as médias observadas para a dimensão atual
    mu_config1 <- mean(estudo_piloto$Fbest[estudo_piloto$Dimensao == dim &
                                           estudo_piloto$Configuracao == "Config1"])
    mu_config2 <- mean(estudo_piloto$Fbest[estudo_piloto$Dimensao == dim &
                                           estudo_piloto$Configuracao == "Config2"])
    # Média global
    mu <- mean(c(mu_config1, mu_config2))

    # Calcula o NCP
    ncp <- (n*((mu_config1 - mu)^2 + (mu_config2 - mu)^2)) / (sd^2)

    F.crit <- qf(alpha, df1, df2, lower.tail = FALSE)

    power <- pf(F.crit, df1, df2, ncp, lower.tail = FALSE)
    
    if (n >= n_max) {
      break
    } else {
      n = n + 1
    }
  }

  # Armazena os resultados no dataframe
  repeticoes <- rbind(repeticoes, data.frame(Dimensao = dim, Repeticoes = n, Poder = power))
  # Salvar resultados em um arquivo CSV
  write.csv(repeticoes, "repeticoes.csv", row.names = FALSE)

}
```

<!-- ### 2. Descrição do conjunto de dados -->

<!-- Os dados utilizados neste estudo são referentes aos preços de fechamento de cinco ações, extraídos de um arquivo CSV. Cada arquivo contém 36 linhas e 5 colunas, onde: -->

<!-- - Cada linha representa o preço de fechamento mensal das ações, com a linha 1 correspondendo ao mês mais recente e a linha 36 ao mês mais distante. -->
<!-- - Cada coluna representa uma das cinco ações analisadas. -->
<!-- - O conteúdo da posição (i, j) refere-se ao preço de fechamento da ação j no mês i. -->

<!-- Com base nesse conjunto de dados, foi calculado o retorno mensal de cada ação utilizando a fórmula: -->

<!-- \[ -->
<!-- \text{Retorno} = \frac{\text{Preço no mês atual} - \text{Preço no mês anterior}}{\text{Preço no mês anterior}} -->
<!-- \] -->

<!-- O primeiro mês de cada coluna foi descartado, pois não há um valor anterior para o cálculo do retorno. O processo resultou em uma tabela com os retornos mensais das cinco ações. -->


<!-- ```{r loaddata,message=FALSE} -->
<!-- # Lê o arquivo CSV com os preços de fechamento das ações -->
<!-- precos <- read.csv("DadosAcoesGrupoH.csv", header=FALSE) -->
<!-- colnames(precos) <- paste0("Acao_", 1:5) -->

<!-- # Calcula o retorno mensal de cada ação -->
<!-- # Retorno mensal = (Preço atual - Preço anterior) / Preço anterior -->
<!-- retornos <- precos -->
<!-- for (j in 1:ncol(precos)) { -->
<!--   for (i in 1:(nrow(precos)-1)) { -->
<!--     retornos[i, j] <- ((precos[i, j] - precos[i+1, j])) / precos[i+1, j] -->
<!--   } -->
<!-- } -->

<!-- # Remove a ultima linha, referente ao primeiro mês -->
<!-- retornos <- retornos[-36, ] -->

<!-- # Transforma os dados para visualização -->
<!-- retornos_melt <- melt(retornos, variable.name = "Acoes", value.name = "Retorno") -->
<!-- ``` -->


## 3. Análise Exploratória

Antes de realizar os testes de hipótese, foi realizada uma análise exploratória para obter uma visão geral dos dados. A Figura @ref(fig:dataplot) fornece um gráfico para comparar o desempenho médio das duas configuração por dimensão.


```{r dataplot,fig.width=8,echo=TRUE,message=FALSE,label="dataplot",fig.cap="Desempenho médio das configurações por instância"}
# Load data
data <- read.csv("estudo_piloto_4.csv")
# Aggregate data (algorithm means by instance group)
aggdata <- with(data, aggregate(x = Fbest, by = list(Configuracao, Dimensao), FUN = mean))
names(aggdata) <- c("Configuracao", "Dimensao", "Y")

library(ggplot2)

# png(filename = "../figs/algo_lineplot.png",
#     width = 1000, height = 400,
#     bg = "transparent")
p <- ggplot(aggdata, aes(x = Dimensao,
                         y = Y,
                         group = Configuracao,
                         colour = Configuracao))
p + geom_line(linetype=2) + geom_point(size=5)
# dev.off()
```

A análise gráfica indica que a Configuração X apresenta


## 4. Análise Estatística

Para validar nossas observações iniciais, realizamos um teste RCBD para avaliar as diferenças no desempenho das duas configurações.

```{r fitmodel,results='hold'}
model <- aov(Y~Configuracao+Dimensao, data=aggdata)
summary(model)
summary.lm(model)$r.squared
```

A Figura @ref(modelplot) mostra o modelo resultante do teste estatístico.

```{r modelplot,results='hold',label="modelplot"}
par(mfrow = c(2, 2))
plot(model, pch = 20, las = 1)
```

<!-- Como resultado do teste, obteve-se uma estatística F = 34,45 e um p-valor < 2e-16. -->

<!-- Diante do resultado do teste ANOVA, podemos rejeitar, com nível de significância $\alpha = 0,05$, a hipótese nula de que as médias de retorno das cinco ações são iguais. -->

<!-- Esse resultado indica que nem todas as ações têm retornos médios semelhantes e, para identificar quais ações possuem maior retorno médio, realizamos um teste de Tukey. Os resultados do teste de Turkey são mostrados na Figura @ref{fig:turkey}. -->


<!-- ```{r turkey,results='hold',fig.width=8,echo=TRUE,message=FALSE,fig.cap="Comparações múltiplas - Turkey"} -->
<!-- # Comparações múltiplas -->
<!-- library(multcomp) -->

<!-- mc1    <- glht(model, -->
<!--                linfct = mcp(Acoes = "Tukey")) -->
<!-- summary(mc1) -->
<!-- mc1_CI <- confint(mc1, level = 0.95) -->

<!-- par(mar = c(5, 10, 4, 2)) # margens -->
<!-- plot(mc1_CI, -->
<!--      xlab       = "Retornos", -->
<!--      cex.axis   = 1.2, -->
<!--      cex        = 2, -->
<!--      main = "Comparações Múltiplas - Tukey") -->
<!-- ``` -->

<!-- As comparações mais significativas são entre a Ação 2 e as Ações 1, 3, 4 e 5, todas com p-valor < 0,001, indicando que essas ações têm médias de retorno significativamente diferentes. As Ações 1, 3, 4 e 5, por sua vez, apresentam pelo menos uma diferença não significativa quando comparadas entre si. -->

Confirmando a suspeita inicial, o teste de Tukey indica que a Ação 2 tem o maior retorno médio entre as ações analisadas, visto que as comparações entre a Ação 2 e as outras ações todas mostram diferenças significativas.

<!-- ### 5. Verificação das Premissas do Modelo -->
<!-- The assumptions of your test should also be validated, and possible effects of violations should also be explored. -->
<!-- ```{r shapiro,fig.width=8,echo=TRUE,message=FALSE, fig.cap = "Teste de Shapiro-Wilk"} -->
<!-- # Check normality -->
<!-- shapiro.test(retornos_melt) -->

<!-- library(car) -->
<!-- # png(filename = "../figs/paperqq.png", -->
<!-- #     width = 600, height = 600,  -->
<!-- #     bg = "transparent") -->

<!-- qqPlot(retornos_melt,  -->
<!--        pch = 16,  -->
<!--        lwd = 3,  -->
<!--        cex = 2,  -->
<!--        las = 1) -->
<!-- # dev.off() -->

<!-- ``` -->

<!-- # Check homoscedasticity -->
<!-- fligner.test(Retorno ~ Acoes, -->
<!--              data = retornos_melt) -->

<!-- # png(filename = "../figs/papervar.png", -->
<!-- #     width = 600, height = 600,  -->
<!-- #     bg = "transparent") -->
<!-- plot(x    = my.model$fitted.values, -->
<!--      y    = my.model$residuals, -->
<!--      cex  = 2, -->
<!--      las  = 1, -->
<!--      pch  = 16, -->
<!--      xlab = "Fitted values", -->
<!--      ylab = "Residuals") -->
<!-- grid(nx = NULL, ny = NULL,  -->
<!--      lwd = 2, col = "#44444422") -->
<!-- # dev.off() -->
<!-- ``` -->

<!-- ### 6. Conclusões e Recomendações -->
<!-- The discussion of your results, and the scientific/technical meaning of the effects detected, should be placed here. Always be sure to tie your results back to the original question of interest! -->

<!-- ### 7. Atividades Específicas -->

<!-- - Ana Julia: Design do experimento, análise estatística, análise exploratória e descrição do conjunto de dados (redação e código)  -->
<!-- - Antônio:  -->
<!-- - Melchior: -->

### 8. Referências

[1] Mathews, P.: Sample Size Calculations: Practical Methods for Engineers and Scientists, 1st edn. Matthews Malnar & Bailey Inc., Fairport Harbor (2010)

[2] Campelo, F., Takahashi, F. Sample size estimation for power and accuracy in the experimental comparison of algorithms. J Heuristics 25, 305–338 (2019). https://doi.org/10.1007/s10732-018-9396-7